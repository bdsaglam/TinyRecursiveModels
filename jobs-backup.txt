# ============================================================================
# BUG FIX VERIFICATION (highest priority)
# Root cause: ACT halting logic caused encoder to be skipped after first batch
# Fix: Force halted=True at start of each batch to ensure fresh encoding
# ============================================================================

# E_fix_v1: Test the gradient flow bug fix - should now have non-zero encoder gradients
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_fix_v1_encoder_grad_fix"

# E_fix_gradclip: Same fix + gradient clipping for stability
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 grad_clip_norm=1.0 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_fix_v1_gradclip"

# ============================================================================
# PHASE 1: OVERFIT VERIFICATION (must pass before anything else)
# Goal: Prove both baseline and encoder can memorize 32 puzzles
# ============================================================================

# E0v2: Baseline TRM (puzzle embeddings) - extended training
# Expected: Should reach 90%+ accuracy if model works
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py --config-name cfg_pretrain_arc_agi_1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E0v2_baseline_overfit_long"

# E1v2: Standard encoder - extended training (same settings as E0v2 for fair comparison)
# NOTE: This failed due to bug - encoder gradients were zero. See E_fix_v1 for fixed version.
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E1v2_encoder_overfit_long"

# E_diag1: Frozen encoder diagnostic - can TRM learn with random fixed encoder?
# If YES: encoder output format is correct, training signal is the issue
# If NO: encoder-TRM interface or representation format is broken
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.freeze_encoder=true epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_diag1_frozen_encoder"

# ============================================================================
# PHASE 2: ENCODER ARCHITECTURE COMPARISON
# Goal: Find the best encoder architecture
# Run these after Phase 1 shows encoder can learn
# ============================================================================

# E2_lpn: LPN-style encoder (deeper, CLS token, mean aggregation)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_standard epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E2_lpn_standard_overfit"

# E3_lpn_var: LPN Variational encoder (per-demo VAE)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_variational epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E3_lpn_variational_overfit"

# ============================================================================
# PHASE 3: ENCODER LEARNING RATE EXPERIMENTS (based on E_diag1 findings)
# Key insight: Frozen encoder works, so encoder is changing too fast for TRM
# ============================================================================

# E_enc_lr_0.1: Encoder 10x slower than TRM (encoder_lr = 1e-5)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 encoder_lr_scale=0.1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_enc_lr_0.1"

# E_enc_lr_0.01: Encoder 100x slower than TRM (encoder_lr = 1e-6)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 encoder_lr_scale=0.01 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_enc_lr_0.01"

# E_enc_lr_0.001: Encoder 1000x slower (nearly frozen, encoder_lr = 1e-7)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 encoder_lr_scale=0.001 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_enc_lr_0.001"

# ============================================================================
# PHASE 4: HYPERPARAMETER TUNING
# Goal: Improve training stability and convergence
# ============================================================================

# E4_lower_lr: Standard encoder with lower learning rate
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 lr=3e-5 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E4_encoder_lr3e5"

# E5_grad_clip: Standard encoder with gradient clipping
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 grad_clip_norm=1.0 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E5_encoder_gradclip1" [1237815]

# E7_attention: Attention pooling only (isolate this variable from E6)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_pooling_method=attention epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E7_attention_pooling"

# E6_combined: Lower LR + grad clip + attention pooling
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 lr=3e-5 grad_clip_norm=1.0 arch.encoder_pooling_method=attention epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E6_encoder_combined"

# ============================================================================
# PHASE 5: STAGED TRAINING (based on E_diag1 success)
# Key insight: Frozen encoder achieves 57.8% exact accuracy, trainable gets 0%
# Strategy: Freeze encoder initially, let TRM learn, then slowly unfreeze
# ============================================================================

# E_staged_5k: Freeze encoder for 5000 steps, then unfreeze with encoder_lr_scale=0.01
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 encoder_freeze_steps=5000 encoder_lr_scale=0.01 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_staged_5k_lr0.01"

# E_staged_10k: Freeze encoder for 10000 steps (longer warmup)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 encoder_freeze_steps=10000 encoder_lr_scale=0.01 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_staged_10k_lr0.01"

# E_staged_5k_slower: Freeze 5000 steps, then very slow encoder (lr_scale=0.001)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 encoder_freeze_steps=5000 encoder_lr_scale=0.001 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_staged_5k_lr0.001"