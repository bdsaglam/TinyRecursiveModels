# ============================================================================
# TRANSFER LEARNING DEBUG EXPERIMENTS
# Goal: Quick sense-check of pretrained decoder initialization
# Strategy: Overfit to small subset (32 groups) before full training
# ============================================================================


# D2: Pretrained decoder + freeze decoder (5k steps)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 freeze_decoder_steps=5000 max_train_groups=32 max_eval_groups=32 arch.num_act_steps=4 epochs=50000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D2_pretrained_freeze5k"

# D4: Pretrained decoder (freeze 1k steps) + LPN variational encoder
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 arch.encoder_type=lpn_variational freeze_decoder_steps=1000 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D4_pretrained_lpn_var" 

# D1: Pretrained decoder (no freeze) + training
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D1_pretrained_full"

# Baseline: No pretrained decoder (for comparison)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 max_train_groups=32 max_eval_groups=32 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D0_baseline_no_pretrain"

# D4-retry: Pretrained decoder (100 steps frozen) + LPN variational encoder
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 arch.encoder_type=lpn_variational freeze_decoder_steps=100 arch.num_act_steps=4 epochs=50000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D4_pretrained_lpn_var2" 

# D5: Hybrid variational encoder (4L grid + cross-attention set + VAE)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=100 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D5_hybrid_4L"

# D5a: Hybrid with 8 layers (vs 4) - test if deeper helps (smaller batch for memory)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=8 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=100 max_train_groups=32 max_eval_groups=32 global_batch_size=64 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D5a_hybrid_8L"

# D5b: Hybrid with 1 set encoder layer (vs 2) - test cross-attention depth
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=1 freeze_decoder_steps=100 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D5b_hybrid_set1"

# D5c: Hybrid with no freeze (like D1) - test if freeze helps hybrid
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D5c_hybrid_nofreeze"

# D1-full: Pretrained decoder (no freeze) + full training
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.num_act_steps=4 +project_name="mmi-714-tl-debug" +run_name="D1-full_nofreeze"

# ============================================================================
# D6: HYBRID ENCODER RE-RUN WITH STABILITY FIXES
# Previous D5 experiments collapsed due to:
# 1. logvar overflow (fixed: zeros init + clamping)
# 2. Set encoder output drift (fixed: RMS norm before variational bottleneck)
# ============================================================================

# D6: Hybrid 4L with no freeze (best config from D5c before collapse)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D6_hybrid_nofreeze_fixed"

# D6a: Hybrid 4L with freeze 100 steps (compare to D6)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=100 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="D6a_hybrid_freeze100_fixed"

# ============================================================================
# H: HYBRID ENCODER HYPERPARAMETER EXPLORATION (32 groups overfit)
# Compare hybrid_standard vs hybrid_variational with different hyperparams
# ============================================================================

# --- Hybrid Standard (no VAE) baseline ---
# H1: Hybrid Standard 4L (baseline non-variational)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_standard arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="H1_hybrid_std_4L"

# H2: Hybrid Standard 2L (shallower encoder)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_standard arch.encoder_num_layers=2 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="H2_hybrid_std_2L"

# H3: Hybrid Standard 8L (deeper encoder, smaller batch)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_standard arch.encoder_num_layers=8 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=64 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="H3_hybrid_std_8L"

# H4: Hybrid Standard 4L with 1 set layer (shallower cross-attention)
[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_standard arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=1 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="H4_hybrid_std_set1" [1790890]

# --- Hybrid Variational depth comparison ---
# H5: Hybrid Variational 2L (shallower encoder)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=2 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="H5_hybrid_var_2L"

# H6: Hybrid Variational 8L (deeper encoder, smaller batch)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=8 arch.encoder_norm_style=pre arch.encoder_set_layers=2 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=64 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="H6_hybrid_var_8L"

# H7: Hybrid Variational 4L with 1 set layer (compare to D6 which has 2)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=1 freeze_decoder_steps=0 max_train_groups=32 max_eval_groups=32 global_batch_size=128 arch.num_act_steps=4 epochs=20000 eval_interval=5000 +project_name="mmi-714-tl-debug" +run_name="H7_hybrid_var_set1"


# ============================================================================
# ACT MODE COMPARISON EXPERIMENTS
# Goal: Test original TRM training dynamics with encoder
# Strategy: Overfit to small subset (32 groups) before full training
# All experiments use PRETRAINED DECODER (proven to improve training)
# ============================================================================
#
# ORIGINAL MODE (pretrain_encoder_original.py):
# - ONE forward per batch, carry persists across batches
# - Dynamic halting with Q-head exploration
# - Encoder called once when sample starts (cached in carry)
# - Matches original TRM paper's training dynamics
#
# Key metrics:
# - train/q_halt_accuracy: Does Q-head learn to predict correctness?
# - train/steps: Average ACT steps used
# - eval/accuracy and eval/exact_accuracy
# ============================================================================

# Pretrained decoder path (used in all experiments)
# /home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071


# ============================================================================
# O: ORIGINAL MODE EXPERIMENTS (32 groups overfit)
# Test original TRM training dynamics with encoder
# ============================================================================

# O1: Original mode baseline (exploration 0.5)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000 +project_name="mmi-714-act-mode" +run_name="O1_original_baseline"

# O2: Original mode with lower exploration (0.3)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_exploration_prob=0.3 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000 +project_name="mmi-714-act-mode" +run_name="O2_original_explore0.3"

# O3: Original mode with higher exploration (0.7)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_exploration_prob=0.7 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000 +project_name="mmi-714-act-mode" +run_name="O3_original_explore0.7"


# ============================================================================
# E: ENCODER TYPE COMPARISON WITH ORIGINAL MODE
# Test different encoder architectures with original training dynamics
# ============================================================================

# E1: Original mode + Hybrid Standard encoder
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_standard arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 max_train_groups=32 max_eval_groups=32 global_batch_size=128 epochs=20000 eval_interval=5000 +project_name="mmi-714-act-mode" +run_name="E1_original_hybrid_std"

# E2: Original mode + Hybrid Variational encoder
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 max_train_groups=32 max_eval_groups=32 global_batch_size=128 epochs=20000 eval_interval=5000 +project_name="mmi-714-act-mode" +run_name="E2_original_hybrid_var"

# E3: Original mode + LPN Variational encoder
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=lpn_variational max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000 +project_name="mmi-714-act-mode" +run_name="E3_original_lpn_var"

