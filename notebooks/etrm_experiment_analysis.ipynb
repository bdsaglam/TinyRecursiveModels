{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETRM Experiment Analysis\n",
    "\n",
    "This notebook fetches all experiments from a W&B project and compares them by key metrics.\n",
    "\n",
    "**Key Metrics Categories:**\n",
    "1. **Primary Success Indicators**: train/exact_accuracy, encoder_cross_sample_var, grad/encoder_norm\n",
    "2. **Secondary Indicators**: train/steps, q_halt_accuracy, lm_loss\n",
    "3. **Encoder Health**: encoder output statistics\n",
    "4. **Evaluation Performance**: all.exact_accuracy, ARC/pass@k\n",
    "\n",
    "See `docs/experiment-guide.md` for detailed metric explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}' if abs(x) < 1000 else f'{x:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USER CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "ENTITY = \"bdsaglam\"\n",
    "PROJECT = \"etrm-semi-final-subset-eval\"\n",
    "\n",
    "# Optional: Filter runs by state\n",
    "# Options: \"finished\", \"running\", \"crashed\", \"failed\", or None for all\n",
    "STATE_FILTER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Experiments from W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Fetch runs\n",
    "print(f\"Fetching runs from {ENTITY}/{PROJECT}...\")\n",
    "runs = api.runs(\n",
    "    f\"{ENTITY}/{PROJECT}\",\n",
    "    filters={\"state\": STATE_FILTER} if STATE_FILTER else {}\n",
    ")\n",
    "\n",
    "print(f\"Found {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Key Metrics\n",
    "\n",
    "We extract the **final value** (latest logged value) for each metric from each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get(d, key, default=np.nan):\n",
    "    \"\"\"Safely get value from dict, return default if missing.\"\"\"\n",
    "    try:\n",
    "        val = d.get(key, default)\n",
    "        return val if val is not None else default\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "# Extract data from runs\n",
    "data = []\n",
    "for run in runs:\n",
    "    # Get summary (final values) and config\n",
    "    summary = run.summary._json_dict\n",
    "    config = {k: v for k, v in run.config.items() if not k.startswith('_')}\n",
    "    \n",
    "    row = {\n",
    "        # Run metadata\n",
    "        'name': run.name,\n",
    "        'display_name': run.name if not run.name.startswith('E') else run.name,  # Short name\n",
    "        'state': run.state,\n",
    "        'created_at': run.created_at,\n",
    "        'duration_hrs': (run.summary.get('_runtime', 0) / 3600),\n",
    "        'steps': safe_get(summary, '_step', 0),\n",
    "        \n",
    "        # Primary Success Indicators\n",
    "        'train_exact_acc': safe_get(summary, 'train/exact_accuracy') * 100,  # Convert to %\n",
    "        'encoder_var': safe_get(summary, 'train/encoder_cross_sample_var'),\n",
    "        'grad_encoder': safe_get(summary, 'grad/encoder_norm'),\n",
    "        \n",
    "        # Secondary Indicators\n",
    "        'train_acc': safe_get(summary, 'train/accuracy') * 100,\n",
    "        'train_steps': safe_get(summary, 'train/steps'),\n",
    "        'q_halt_acc': safe_get(summary, 'train/q_halt_accuracy') * 100,\n",
    "        'lm_loss': safe_get(summary, 'train/lm_loss'),\n",
    "        'q_halt_loss': safe_get(summary, 'train/q_halt_loss'),\n",
    "        \n",
    "        # Gradient metrics\n",
    "        'grad_inner': safe_get(summary, 'grad/inner_norm'),\n",
    "        'grad_total': safe_get(summary, 'grad/total_norm'),\n",
    "        \n",
    "        # Encoder statistics\n",
    "        'encoder_mean': safe_get(summary, 'train/encoder_output_mean'),\n",
    "        'encoder_std': safe_get(summary, 'train/encoder_output_std'),\n",
    "        'encoder_norm': safe_get(summary, 'train/encoder_output_norm'),\n",
    "        'encoder_token_std': safe_get(summary, 'train/encoder_token_std'),\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        'eval_exact_acc': safe_get(summary, 'all.exact_accuracy') * 100,\n",
    "        'eval_acc': safe_get(summary, 'all.accuracy') * 100,\n",
    "        'arc_pass1': safe_get(summary, 'ARC/pass@1') * 100,\n",
    "        'arc_pass2': safe_get(summary, 'ARC/pass@2') * 100,\n",
    "        'arc_pass10': safe_get(summary, 'ARC/pass@10') * 100,\n",
    "        'arc_pass1000': safe_get(summary, 'ARC/pass@1000') * 100,\n",
    "        \n",
    "        # Key config parameters\n",
    "        'encoder_type': config.get('arch', {}).get('encoder_type', 'standard'),\n",
    "        'encoder_layers': config.get('arch', {}).get('encoder_num_layers', 2),\n",
    "        'halt_max_steps': config.get('arch', {}).get('halt_max_steps', 16),\n",
    "        'halt_explore_prob': config.get('arch', {}).get('halt_exploration_prob', 0.5),\n",
    "        'lr': config.get('optim', {}).get('learning_rate', np.nan),\n",
    "        'batch_size': config.get('global_batch_size', np.nan),\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort by creation time (newest first)\n",
    "df = df.sort_values('created_at', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Extracted metrics from {len(df)} runs\")\n",
    "print(f\"\\nRun states: {df['state'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Overview: All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info for all runs\n",
    "overview = df[[\n",
    "    'display_name', 'state', 'steps', 'duration_hrs',\n",
    "    'train_exact_acc', 'encoder_var', 'grad_encoder',\n",
    "    'encoder_type', 'halt_max_steps', 'halt_explore_prob'\n",
    "]].copy()\n",
    "\n",
    "overview.columns = [\n",
    "    'Name', 'State', 'Steps', 'Hours',\n",
    "    'Train EM%', 'Enc Var', 'Grad Enc',\n",
    "    'Encoder', 'Max Steps', 'Explore Prob'\n",
    "]\n",
    "\n",
    "display(overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check Summary\n",
    "\n",
    "Classify experiments as **Healthy**, **Warning**, or **Failed** based on key metrics:\n",
    "\n",
    "- ‚úÖ **Healthy**: `grad_encoder > 0.2` AND `encoder_var > 0.15`\n",
    "- ‚ö†Ô∏è **Warning**: `grad_encoder > 0.05` OR `encoder_var > 0.1`\n",
    "- ‚ùå **Failed**: `grad_encoder < 0.05` OR `encoder_var < 0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def health_status(row):\n",
    "    \"\"\"Determine health status based on gradient flow and encoder variance.\"\"\"\n",
    "    grad_enc = row['grad_encoder']\n",
    "    enc_var = row['encoder_var']\n",
    "    \n",
    "    if pd.isna(grad_enc) or pd.isna(enc_var) or isinstance(grad_enc, str) or isinstance(enc_var, str):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Failed: gradient starvation or collapsed encoder\n",
    "    if grad_enc < 0.05 or enc_var < 0.1:\n",
    "        return '‚ùå Failed'\n",
    "    \n",
    "    # Healthy: good gradients and diversity\n",
    "    if grad_enc > 0.2 and enc_var > 0.15:\n",
    "        return '‚úÖ Healthy'\n",
    "    \n",
    "    # Warning: borderline\n",
    "    return '‚ö†Ô∏è Warning'\n",
    "\n",
    "df['health'] = df.apply(health_status, axis=1)\n",
    "\n",
    "# Show health summary\n",
    "print(\"=\" * 50)\n",
    "print(\"HEALTH CHECK SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(df['health'].value_counts())\n",
    "print()\n",
    "\n",
    "# Show problematic runs\n",
    "failed_runs = df[df['health'] == '‚ùå Failed']\n",
    "if len(failed_runs) > 0:\n",
    "    print(\"\\nFailed Runs (gradient starvation or collapsed encoder):\")\n",
    "    display(failed_runs[[\n",
    "        'display_name', 'state', 'train_exact_acc', \n",
    "        'encoder_var', 'grad_encoder', 'encoder_type'\n",
    "    ]])\n",
    "\n",
    "warning_runs = df[df['health'] == '‚ö†Ô∏è Warning']\n",
    "if len(warning_runs) > 0:\n",
    "    print(\"\\nWarning Runs (borderline metrics):\")\n",
    "    display(warning_runs[[\n",
    "        'display_name', 'state', 'train_exact_acc', \n",
    "        'encoder_var', 'grad_encoder', 'encoder_type'\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Metrics Comparison\n",
    "\n",
    "Focus on the 3 most important metrics for overfit experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(\n",
    "    axis=0,\n",
    "    how=\"any\",\n",
    "    subset=[\n",
    "        \"train_exact_acc\",\n",
    "        \"encoder_var\",\n",
    "        \"grad_encoder\",\n",
    "        \"grad_inner\",\n",
    "        \"grad_total\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary metrics table\n",
    "primary = df[[\n",
    "    'display_name', 'state', 'steps',\n",
    "    'train_exact_acc', 'encoder_var', 'grad_encoder',\n",
    "    'health', 'encoder_type'\n",
    "]].copy()\n",
    "\n",
    "primary.columns = [\n",
    "    'Name', 'State', 'Steps',\n",
    "    'Train EM%', 'Encoder Var', 'Grad Encoder',\n",
    "    'Health', 'Encoder Type'\n",
    "]\n",
    "\n",
    "# Sort by train exact accuracy\n",
    "primary = primary.sort_values('Train EM%', ascending=False)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PRIMARY METRICS (sorted by Train Exact Match %)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTarget: Train EM% > 90, Encoder Var > 0.3, Grad Encoder > 0.2\\n\")\n",
    "\n",
    "display(primary)\n",
    "\n",
    "# Highlight best performer\n",
    "best = primary.iloc[0]\n",
    "print(f\"\\nüèÜ Best performer: {best['Name']} with {best['Train EM%']:.1f}% exact accuracy\")\n",
    "print(f\"   Encoder Var: {best['Encoder Var']:.3f}, Grad Encoder: {best['Grad Encoder']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Metrics: Learning Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary = df[[\n",
    "    'display_name', 'state',\n",
    "    'train_acc', 'train_steps', 'q_halt_acc',\n",
    "    'lm_loss', 'q_halt_loss',\n",
    "    'halt_max_steps', 'halt_explore_prob'\n",
    "]].copy()\n",
    "\n",
    "secondary.columns = [\n",
    "    'Name', 'State',\n",
    "    'Train Acc%', 'Avg Steps', 'Q-Halt Acc%',\n",
    "    'LM Loss', 'Q-Halt Loss',\n",
    "    'Max Steps', 'Explore Prob'\n",
    "]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECONDARY METRICS: Learning Dynamics\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTarget: Train Acc% > 90, Q-Halt Acc% > 85, LM Loss < 0.3\\n\")\n",
    "\n",
    "display(secondary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_stats = df[[\n",
    "    'display_name', 'state',\n",
    "    'encoder_var', 'encoder_mean', 'encoder_std',\n",
    "    'encoder_norm', 'encoder_token_std',\n",
    "    'encoder_type', 'encoder_layers'\n",
    "]].copy()\n",
    "\n",
    "encoder_stats.columns = [\n",
    "    'Name', 'State',\n",
    "    'Cross-Sample Var', 'Mean', 'Std',\n",
    "    'Norm', 'Token Std',\n",
    "    'Type', 'Layers'\n",
    "]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ENCODER OUTPUT STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nCross-Sample Var (diversity): >0.3 is good, <0.1 is bad\\n\")\n",
    "\n",
    "display(encoder_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Flow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradient ratios\n",
    "df['grad_enc_ratio'] = df['grad_encoder'] / df['grad_total']\n",
    "df['grad_dec_ratio'] = df['grad_inner'] / df['grad_total']\n",
    "\n",
    "gradient = df[[\n",
    "    'display_name', 'state',\n",
    "    'grad_encoder', 'grad_inner', 'grad_total',\n",
    "    'grad_enc_ratio', 'grad_dec_ratio',\n",
    "    'encoder_type'\n",
    "]].copy()\n",
    "\n",
    "gradient.columns = [\n",
    "    'Name', 'State',\n",
    "    'Grad Enc', 'Grad Dec', 'Grad Total',\n",
    "    'Enc Ratio', 'Dec Ratio',\n",
    "    'Encoder Type'\n",
    "]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GRADIENT FLOW ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nHealthy Enc Ratio: 0.2-0.5 (encoder getting 20-50% of gradients)\")\n",
    "print(\"Gradient starvation: Enc Ratio < 0.1\\n\")\n",
    "\n",
    "display(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Performance\n",
    "\n",
    "During overfit phase, eval set = train set, so these should match training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = df[[\n",
    "    'display_name', 'state',\n",
    "    'eval_exact_acc', 'eval_acc',\n",
    "    'arc_pass1', 'arc_pass2', 'arc_pass10', 'arc_pass1000',\n",
    "    'train_exact_acc'  # For comparison\n",
    "]].copy()\n",
    "\n",
    "# Calculate voting benefit\n",
    "evaluation['voting_benefit'] = evaluation['arc_pass1'] / evaluation['eval_exact_acc'].replace(0, np.nan)\n",
    "\n",
    "evaluation.columns = [\n",
    "    'Name', 'State',\n",
    "    'Eval EM%', 'Eval Acc%',\n",
    "    'Pass@1%', 'Pass@2%', 'Pass@10%', 'Pass@1000%',\n",
    "    'Train EM%', 'Voting Benefit'\n",
    "]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nVoting Benefit: Pass@1 / Eval EM (how much voting helps)\\n\")\n",
    "\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to runs with meaningful data\n",
    "df_plot = df[df['steps'] > 0].copy()\n",
    "\n",
    "if len(df_plot) == 0:\n",
    "    print(\"No runs with data to plot\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(f'ETRM Experiment Comparison: {PROJECT}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Train Exact Accuracy vs Encoder Variance\n",
    "    ax = axes[0, 0]\n",
    "    scatter = ax.scatter(\n",
    "        df_plot['encoder_var'],\n",
    "        df_plot['train_exact_acc'],\n",
    "        c=df_plot['grad_encoder'],\n",
    "        s=100,\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax.axhline(y=90, color='green', linestyle='--', alpha=0.5, label='Target (90%)')\n",
    "    ax.axvline(x=0.3, color='blue', linestyle='--', alpha=0.5, label='Good Var (0.3)')\n",
    "    ax.axvline(x=0.1, color='red', linestyle='--', alpha=0.5, label='Bad Var (0.1)')\n",
    "    ax.set_xlabel('Encoder Cross-Sample Variance')\n",
    "    ax.set_ylabel('Train Exact Match %')\n",
    "    ax.set_title('Success vs Encoder Diversity')\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='Grad Encoder')\n",
    "    \n",
    "    # 2. Gradient Encoder vs Train Accuracy\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(df_plot['grad_encoder'], df_plot['train_exact_acc'], s=100, alpha=0.6)\n",
    "    ax.axhline(y=90, color='green', linestyle='--', alpha=0.5, label='Target (90%)')\n",
    "    ax.axvline(x=0.2, color='blue', linestyle='--', alpha=0.5, label='Healthy (0.2)')\n",
    "    ax.axvline(x=0.05, color='red', linestyle='--', alpha=0.5, label='Starvation (0.05)')\n",
    "    ax.set_xlabel('Gradient Encoder Norm')\n",
    "    ax.set_ylabel('Train Exact Match %')\n",
    "    ax.set_title('Gradient Flow vs Success')\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Gradient Ratio Distribution\n",
    "    ax = axes[0, 2]\n",
    "    df_plot_valid = df_plot[df_plot['grad_enc_ratio'].notna()]\n",
    "    if len(df_plot_valid) > 0:\n",
    "        ax.bar(range(len(df_plot_valid)), df_plot_valid['grad_enc_ratio'], alpha=0.6)\n",
    "        ax.axhline(y=0.2, color='blue', linestyle='--', alpha=0.5, label='Healthy min (0.2)')\n",
    "        ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='Warning (0.1)')\n",
    "        ax.set_xticks(range(len(df_plot_valid)))\n",
    "        ax.set_xticklabels(df_plot_valid['display_name'], rotation=45, ha='right', fontsize=8)\n",
    "        ax.set_ylabel('Encoder Gradient Ratio')\n",
    "        ax.set_title('Encoder Gradient Share')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Training Progress (Token vs Exact Accuracy)\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(df_plot['train_acc'], df_plot['train_exact_acc'], s=100, alpha=0.6)\n",
    "    ax.plot([0, 100], [0, 100], 'r--', alpha=0.3, label='Perfect correlation')\n",
    "    ax.set_xlabel('Train Token Accuracy %')\n",
    "    ax.set_ylabel('Train Exact Match %')\n",
    "    ax.set_title('Token Acc vs Exact Match')\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Halting Behavior\n",
    "    ax = axes[1, 1]\n",
    "    colors = df_plot['halt_max_steps'].map({8: 'red', 16: 'blue', 32: 'green'})\n",
    "    ax.scatter(\n",
    "        df_plot['train_steps'],\n",
    "        df_plot['train_exact_acc'],\n",
    "        c=colors,\n",
    "        s=100,\n",
    "        alpha=0.6\n",
    "    )\n",
    "    ax.set_xlabel('Average ACT Steps')\n",
    "    ax.set_ylabel('Train Exact Match %')\n",
    "    ax.set_title('Halting Behavior vs Success')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Add legend for max_steps\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='red', alpha=0.6, label='max_steps=8'),\n",
    "        Patch(facecolor='blue', alpha=0.6, label='max_steps=16'),\n",
    "        Patch(facecolor='green', alpha=0.6, label='max_steps=32')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=8)\n",
    "    \n",
    "    # 6. Encoder Type Comparison\n",
    "    ax = axes[1, 2]\n",
    "    encoder_comparison = df_plot.groupby('encoder_type')['train_exact_acc'].agg(['mean', 'max', 'count'])\n",
    "    if len(encoder_comparison) > 0:\n",
    "        x = range(len(encoder_comparison))\n",
    "        ax.bar(x, encoder_comparison['mean'], alpha=0.6, label='Mean')\n",
    "        ax.scatter(x, encoder_comparison['max'], color='red', s=100, zorder=5, label='Max')\n",
    "        ax.axhline(y=90, color='green', linestyle='--', alpha=0.5, label='Target (90%)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(encoder_comparison.index, rotation=45, ha='right', fontsize=8)\n",
    "        ax.set_ylabel('Train Exact Match %')\n",
    "        ax.set_title('Encoder Type Comparison')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Annotate with counts\n",
    "        for i, (idx, row) in enumerate(encoder_comparison.iterrows()):\n",
    "            ax.text(i, row['mean'] + 2, f\"n={int(row['count'])}\", \n",
    "                   ha='center', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Comparison\n",
    "\n",
    "Compare key hyperparameters across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_comparison = df[[\n",
    "    'display_name', 'state', 'train_exact_acc',\n",
    "    'encoder_type', 'encoder_layers',\n",
    "    'halt_max_steps', 'halt_explore_prob',\n",
    "    'lr', 'batch_size'\n",
    "]].copy()\n",
    "\n",
    "config_comparison.columns = [\n",
    "    'Name', 'State', 'Train EM%',\n",
    "    'Encoder Type', 'Enc Layers',\n",
    "    'Max Steps', 'Explore Prob',\n",
    "    'LR', 'Batch Size'\n",
    "]\n",
    "\n",
    "# Sort by performance\n",
    "config_comparison = config_comparison.sort_values('Train EM%', ascending=False)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "display(config_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Frontier Analysis\n",
    "\n",
    "Identify experiments that are **Pareto optimal** - experiments that represent the best trade-offs between multiple objectives.\n",
    "\n",
    "An experiment is on the Pareto frontier if:\n",
    "1. It's the **best** in at least one significant metric, OR\n",
    "2. There's **no other experiment** that is better or equal in ALL significant metrics\n",
    "\n",
    "**Significant Metrics** (for Pareto analysis):\n",
    "- `train_exact_acc` (higher is better) - Training success metric\n",
    "- `arc_pass1` (higher is better) - **Test set performance (generalization)**\n",
    "- `encoder_var` (higher is better) - Encoder diversity\n",
    "- `grad_encoder` (higher is better) - Gradient flow to encoder\n",
    "- `lm_loss` (lower is better) - Prediction quality\n",
    "\n",
    "These experiments represent different trade-offs and should be considered for further analysis.\n",
    "\n",
    "**Note**: During overfit phase, test set = train set (32 groups), but `arc_pass1` uses voting which can reveal better generalization patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pareto_optimal(df, metrics_to_maximize, metrics_to_minimize):\n",
    "    \"\"\"\n",
    "    Identify Pareto optimal solutions.\n",
    "    \n",
    "    An experiment is Pareto optimal if no other experiment dominates it\n",
    "    (i.e., is better or equal in all objectives).\n",
    "    \"\"\"\n",
    "    pareto_mask = np.ones(len(df), dtype=bool)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if not pareto_mask[i]:\n",
    "            continue\n",
    "        \n",
    "        # Check if experiment i is dominated by any other experiment\n",
    "        for j in range(len(df)):\n",
    "            if i == j or not pareto_mask[j]:\n",
    "                continue\n",
    "            \n",
    "            # Check dominance: j dominates i if j is >= i in all maximize metrics\n",
    "            # and <= i in all minimize metrics\n",
    "            dominates = True\n",
    "            strictly_better = False\n",
    "            \n",
    "            # Check maximize metrics (higher is better)\n",
    "            for metric in metrics_to_maximize:\n",
    "                val_i = df.iloc[i][metric]\n",
    "                val_j = df.iloc[j][metric]\n",
    "                \n",
    "                # Handle NaN values\n",
    "                if pd.isna(val_i) and pd.isna(val_j):\n",
    "                    continue\n",
    "                if pd.isna(val_i):\n",
    "                    dominates = False\n",
    "                    break\n",
    "                if pd.isna(val_j):\n",
    "                    continue\n",
    "                \n",
    "                if val_j < val_i:\n",
    "                    dominates = False\n",
    "                    break\n",
    "                if val_j > val_i:\n",
    "                    strictly_better = True\n",
    "            \n",
    "            if not dominates:\n",
    "                continue\n",
    "            \n",
    "            # Check minimize metrics (lower is better)\n",
    "            for metric in metrics_to_minimize:\n",
    "                val_i = df.iloc[i][metric]\n",
    "                val_j = df.iloc[j][metric]\n",
    "                \n",
    "                # Handle NaN values\n",
    "                if pd.isna(val_i) and pd.isna(val_j):\n",
    "                    continue\n",
    "                if pd.isna(val_i):\n",
    "                    dominates = False\n",
    "                    break\n",
    "                if pd.isna(val_j):\n",
    "                    continue\n",
    "                \n",
    "                if val_j > val_i:\n",
    "                    dominates = False\n",
    "                    break\n",
    "                if val_j < val_i:\n",
    "                    strictly_better = True\n",
    "            \n",
    "            # If j dominates i and is strictly better in at least one metric\n",
    "            if dominates and strictly_better:\n",
    "                pareto_mask[i] = False\n",
    "                break\n",
    "    \n",
    "    return pareto_mask\n",
    "\n",
    "# Define metrics for Pareto analysis\n",
    "metrics_to_maximize = ['train_exact_acc', 'arc_pass1', 'encoder_var', 'grad_encoder']\n",
    "metrics_to_minimize = ['lm_loss']\n",
    "\n",
    "# Filter to experiments with valid data\n",
    "df_pareto = df[df['steps'] > 0].copy()\n",
    "\n",
    "if len(df_pareto) > 0:\n",
    "    # Identify Pareto optimal experiments\n",
    "    pareto_mask = is_pareto_optimal(df_pareto, metrics_to_maximize, metrics_to_minimize)\n",
    "    df_pareto['is_pareto'] = pareto_mask\n",
    "    \n",
    "    pareto_experiments = df_pareto[df_pareto['is_pareto']].copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PARETO FRONTIER ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nFound {len(pareto_experiments)} Pareto optimal experiments out of {len(df_pareto)} total\")\n",
    "    print(\"\\nThese experiments represent the best trade-offs between:\")\n",
    "    print(\"  ‚Ä¢ Train Exact Match % (maximize)\")\n",
    "    print(\"  ‚Ä¢ Test Set Pass@1 % (maximize - generalization)\")\n",
    "    print(\"  ‚Ä¢ Encoder Diversity (maximize)\")\n",
    "    print(\"  ‚Ä¢ Gradient Flow (maximize)\")\n",
    "    print(\"  ‚Ä¢ LM Loss (minimize)\")\n",
    "    print()\n",
    "    \n",
    "    # Show Pareto optimal experiments\n",
    "    pareto_table = pareto_experiments[[\n",
    "        'display_name', 'state',\n",
    "        'train_exact_acc', 'arc_pass1', 'encoder_var', 'grad_encoder', 'lm_loss',\n",
    "        'encoder_type', 'halt_max_steps', 'health'\n",
    "    ]].copy()\n",
    "    \n",
    "    pareto_table.columns = [\n",
    "        'Name', 'State',\n",
    "        'Train EM%', 'Test Pass@1%', 'Enc Var', 'Grad Enc', 'LM Loss',\n",
    "        'Encoder Type', 'Max Steps', 'Health'\n",
    "    ]\n",
    "    \n",
    "    # Sort by train exact accuracy\n",
    "    pareto_table = pareto_table.sort_values('Train EM%', ascending=False)\n",
    "    \n",
    "    display(pareto_table)\n",
    "    \n",
    "    # Analyze why each is on the frontier\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"WHY EACH EXPERIMENT IS ON THE PARETO FRONTIER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for idx, row in pareto_experiments.iterrows():\n",
    "        name = row['display_name']\n",
    "        reasons = []\n",
    "        \n",
    "        # Check if best in any metric\n",
    "        for metric in metrics_to_maximize:\n",
    "            if row[metric] == df_pareto[metric].max():\n",
    "                metric_name = {\n",
    "                    'train_exact_acc': 'Train Exact Match',\n",
    "                    'arc_pass1': 'Test Pass@1 (Generalization)',\n",
    "                    'encoder_var': 'Encoder Diversity',\n",
    "                    'grad_encoder': 'Gradient Flow'\n",
    "                }.get(metric, metric)\n",
    "                reasons.append(f\"Best {metric_name} ({row[metric]:.3f})\")\n",
    "        \n",
    "        for metric in metrics_to_minimize:\n",
    "            if row[metric] == df_pareto[metric].min():\n",
    "                metric_name = {'lm_loss': 'LM Loss'}.get(metric, metric)\n",
    "                reasons.append(f\"Best {metric_name} ({row[metric]:.3f})\")\n",
    "        \n",
    "        if reasons:\n",
    "            print(f\"\\n‚ú® {name}: {', '.join(reasons)}\")\n",
    "        else:\n",
    "            print(f\"\\nüîÑ {name}: Optimal trade-off (not dominated by any other experiment)\")\n",
    "            # Show the trade-offs\n",
    "            print(f\"   Train EM: {row['train_exact_acc']:.1f}%, Test Pass@1: {row['arc_pass1']:.1f}%, \"\n",
    "                  f\"Enc Var: {row['encoder_var']:.3f}, Grad Enc: {row['grad_encoder']:.3f}, LM Loss: {row['lm_loss']:.3f}\")\n",
    "else:\n",
    "    print(\"No experiments with data for Pareto analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Pareto frontier\n",
    "if len(df_pareto) > 0 and 'is_pareto' in df_pareto.columns:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Pareto Frontier Analysis: Multi-Objective Trade-offs', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Separate Pareto and dominated experiments\n",
    "    pareto_exps = df_pareto[df_pareto['is_pareto']]\n",
    "    dominated_exps = df_pareto[~df_pareto['is_pareto']]\n",
    "    \n",
    "    # Plot 1: Training vs Test Performance\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(dominated_exps['train_exact_acc'], dominated_exps['arc_pass1'], \n",
    "               s=100, alpha=0.3, color='blue', label='Dominated')\n",
    "    ax.scatter(pareto_exps['train_exact_acc'], pareto_exps['arc_pass1'], \n",
    "               s=150, alpha=0.8, color='red', label='Pareto Optimal', edgecolors='black', linewidth=2)\n",
    "    ax.set_xlabel('Train Exact Match %')\n",
    "    ax.set_ylabel('Test Pass@1 %')\n",
    "    ax.set_title('Training vs Test Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Annotate Pareto points\n",
    "    for idx, row in pareto_exps.iterrows():\n",
    "        ax.annotate(row['display_name'], \n",
    "                   (row['train_exact_acc'], row['arc_pass1']),\n",
    "                   fontsize=8, ha='right')\n",
    "    \n",
    "    # Plot 2: Success vs Encoder Diversity\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(dominated_exps['encoder_var'], dominated_exps['train_exact_acc'], \n",
    "               s=100, alpha=0.3, color='blue', label='Dominated')\n",
    "    ax.scatter(pareto_exps['encoder_var'], pareto_exps['train_exact_acc'], \n",
    "               s=150, alpha=0.8, color='red', label='Pareto Optimal', edgecolors='black', linewidth=2)\n",
    "    ax.axvline(x=0.3, color='green', linestyle='--', alpha=0.5, label='Good Var (0.3)')\n",
    "    ax.axvline(x=0.1, color='orange', linestyle='--', alpha=0.5, label='Bad Var (0.1)')\n",
    "    ax.set_xlabel('Encoder Cross-Sample Variance')\n",
    "    ax.set_ylabel('Train Exact Match %')\n",
    "    ax.set_title('Success vs Encoder Diversity')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Annotate Pareto points\n",
    "    for idx, row in pareto_exps.iterrows():\n",
    "        ax.annotate(row['display_name'], \n",
    "                   (row['encoder_var'], row['train_exact_acc']),\n",
    "                   fontsize=8, ha='right')\n",
    "    \n",
    "    # Plot 3: Success vs Gradient Flow\n",
    "    ax = axes[0, 2]\n",
    "    ax.scatter(dominated_exps['grad_encoder'], dominated_exps['train_exact_acc'], \n",
    "               s=100, alpha=0.3, color='blue', label='Dominated')\n",
    "    ax.scatter(pareto_exps['grad_encoder'], pareto_exps['train_exact_acc'], \n",
    "               s=150, alpha=0.8, color='red', label='Pareto Optimal', edgecolors='black', linewidth=2)\n",
    "    ax.axvline(x=0.2, color='green', linestyle='--', alpha=0.5, label='Healthy (0.2)')\n",
    "    ax.axvline(x=0.05, color='orange', linestyle='--', alpha=0.5, label='Starvation (0.05)')\n",
    "    ax.set_xlabel('Gradient Encoder Norm')\n",
    "    ax.set_ylabel('Train Exact Match %')\n",
    "    ax.set_title('Success vs Gradient Flow')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Annotate Pareto points\n",
    "    for idx, row in pareto_exps.iterrows():\n",
    "        ax.annotate(row['display_name'], \n",
    "                   (row['grad_encoder'], row['train_exact_acc']),\n",
    "                   fontsize=8, ha='right')\n",
    "    \n",
    "    # Plot 4: Test Performance vs Encoder Diversity\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(dominated_exps['encoder_var'], dominated_exps['arc_pass1'], \n",
    "               s=100, alpha=0.3, color='blue', label='Dominated')\n",
    "    ax.scatter(pareto_exps['encoder_var'], pareto_exps['arc_pass1'], \n",
    "               s=150, alpha=0.8, color='red', label='Pareto Optimal', edgecolors='black', linewidth=2)\n",
    "    ax.axvline(x=0.3, color='green', linestyle='--', alpha=0.5, label='Good Var (0.3)')\n",
    "    ax.set_xlabel('Encoder Cross-Sample Variance')\n",
    "    ax.set_ylabel('Test Pass@1 %')\n",
    "    ax.set_title('Test Performance vs Encoder Diversity')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Annotate Pareto points\n",
    "    for idx, row in pareto_exps.iterrows():\n",
    "        ax.annotate(row['display_name'], \n",
    "                   (row['encoder_var'], row['arc_pass1']),\n",
    "                   fontsize=8, ha='right')\n",
    "    \n",
    "    # Plot 5: Encoder Diversity vs Gradient Flow\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(dominated_exps['encoder_var'], dominated_exps['grad_encoder'], \n",
    "               s=100, alpha=0.3, color='blue', label='Dominated')\n",
    "    ax.scatter(pareto_exps['encoder_var'], pareto_exps['grad_encoder'], \n",
    "               s=150, alpha=0.8, color='red', label='Pareto Optimal', edgecolors='black', linewidth=2)\n",
    "    ax.axhline(y=0.2, color='green', linestyle='--', alpha=0.5, label='Healthy Grad (0.2)')\n",
    "    ax.axvline(x=0.3, color='green', linestyle='--', alpha=0.5, label='Good Var (0.3)')\n",
    "    ax.set_xlabel('Encoder Cross-Sample Variance')\n",
    "    ax.set_ylabel('Gradient Encoder Norm')\n",
    "    ax.set_title('Encoder Diversity vs Gradient Flow')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Annotate Pareto points\n",
    "    for idx, row in pareto_exps.iterrows():\n",
    "        ax.annotate(row['display_name'], \n",
    "                   (row['encoder_var'], row['grad_encoder']),\n",
    "                   fontsize=8, ha='right')\n",
    "    \n",
    "    # Plot 6: Success vs Loss\n",
    "    ax = axes[1, 2]\n",
    "    ax.scatter(dominated_exps['lm_loss'], dominated_exps['train_exact_acc'], \n",
    "               s=100, alpha=0.3, color='blue', label='Dominated')\n",
    "    ax.scatter(pareto_exps['lm_loss'], pareto_exps['train_exact_acc'], \n",
    "               s=150, alpha=0.8, color='red', label='Pareto Optimal', edgecolors='black', linewidth=2)\n",
    "    ax.set_xlabel('LM Loss')\n",
    "    ax.set_ylabel('Train Exact Match %')\n",
    "    ax.set_title('Success vs Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Annotate Pareto points\n",
    "    for idx, row in pareto_exps.iterrows():\n",
    "        ax.annotate(row['display_name'], \n",
    "                   (row['lm_loss'], row['train_exact_acc']),\n",
    "                   fontsize=8, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpretation notes\n",
    "    print(\"\\nüìä INTERPRETATION NOTES:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüî¥ Red points = Pareto optimal (best trade-offs)\")\n",
    "    print(\"üîµ Blue points = Dominated (there exists a better experiment in all metrics)\")\n",
    "    print(\"\\nüí° Trade-offs to consider:\")\n",
    "    print(\"   ‚Ä¢ Training success vs Generalization: High train but low test = overfitting\")\n",
    "    print(\"   ‚Ä¢ Success vs Encoder health: High accuracy with low variance = lucky but fragile\")\n",
    "    print(\"   ‚Ä¢ Success vs Gradient flow: High accuracy with low gradients = stagnating\")\n",
    "    print(\"   ‚Ä¢ Encoder diversity vs Gradient flow: Both should be high for healthy learning\")\n",
    "else:\n",
    "    print(\"No data available for Pareto frontier visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Frontier Visualization\n",
    "\n",
    "Visualize the trade-offs between different objectives for Pareto optimal experiments.\n",
    "\n",
    "**Key Plots:**\n",
    "1. **Training vs Test Performance** - Shows train/test trade-off\n",
    "2. **Success vs Encoder Diversity** - Checks for encoder collapse\n",
    "3. **Success vs Gradient Flow** - Identifies gradient starvation\n",
    "4. **Test Performance vs Encoder Diversity** - Generalization vs representation quality\n",
    "5. **Encoder Diversity vs Gradient Flow** - Encoder health correlation\n",
    "6. **Success vs Loss** - Prediction quality vs exact match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "Based on the analysis above, generate recommendations for next experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Overall stats\n",
    "total_runs = len(df)\n",
    "finished_runs = len(df[df['state'] == 'finished'])\n",
    "successful_runs = len(df[df['train_exact_acc'] >= 90])\n",
    "healthy_runs = len(df[df['health'] == '‚úÖ Healthy'])\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"   Total runs: {total_runs}\")\n",
    "print(f\"   Finished: {finished_runs}\")\n",
    "print(f\"   Successful (‚â•90% train EM): {successful_runs}\")\n",
    "print(f\"   Healthy (good gradients & variance): {healthy_runs}\")\n",
    "\n",
    "# Best run\n",
    "if len(df) > 0:\n",
    "    best_run = df.loc[df['train_exact_acc'].idxmax()]\n",
    "    print(f\"\\nüèÜ Best Run: {best_run['display_name']}\")\n",
    "    print(f\"   Train Exact Match: {best_run['train_exact_acc']:.1f}%\")\n",
    "    print(f\"   Encoder Type: {best_run['encoder_type']}\")\n",
    "    print(f\"   Encoder Variance: {best_run['encoder_var']:.3f}\")\n",
    "    print(f\"   Gradient Encoder: {best_run['grad_encoder']:.3f}\")\n",
    "    print(f\"   Health Status: {best_run['health']}\")\n",
    "\n",
    "# Encoder type analysis\n",
    "print(f\"\\nüîß Encoder Type Analysis:\")\n",
    "encoder_perf = df.groupby('encoder_type').agg({\n",
    "    'train_exact_acc': ['mean', 'max', 'count']\n",
    "}).round(2)\n",
    "for encoder_type in encoder_perf.index:\n",
    "    mean_acc = encoder_perf.loc[encoder_type, ('train_exact_acc', 'mean')]\n",
    "    max_acc = encoder_perf.loc[encoder_type, ('train_exact_acc', 'max')]\n",
    "    count = int(encoder_perf.loc[encoder_type, ('train_exact_acc', 'count')])\n",
    "    print(f\"   {encoder_type}: mean={mean_acc:.1f}%, max={max_acc:.1f}% (n={count})\")\n",
    "\n",
    "# Problem detection\n",
    "print(f\"\\n‚ö†Ô∏è Issues Detected:\")\n",
    "gradient_starved = df[df['grad_encoder'] < 0.05]\n",
    "if len(gradient_starved) > 0:\n",
    "    print(f\"   Gradient starvation: {len(gradient_starved)} runs\")\n",
    "    print(f\"      ‚Üí {', '.join(gradient_starved['display_name'].tolist())}\")\n",
    "\n",
    "collapsed_encoder = df[df['encoder_var'] < 0.1]\n",
    "if len(collapsed_encoder) > 0:\n",
    "    print(f\"   Collapsed encoder: {len(collapsed_encoder)} runs\")\n",
    "    print(f\"      ‚Üí {', '.join(collapsed_encoder['display_name'].tolist())}\")\n",
    "\n",
    "not_improving = df[(df['state'] == 'finished') & (df['train_exact_acc'] < 50)]\n",
    "if len(not_improving) > 0:\n",
    "    print(f\"   Failed to learn: {len(not_improving)} runs\")\n",
    "    print(f\"      ‚Üí {', '.join(not_improving['display_name'].tolist())}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "\n",
    "if successful_runs == 0:\n",
    "    print(\"   ‚ùå No successful runs yet (>90% train EM)\")\n",
    "    print(\"   ‚Üí Continue training existing runs or investigate issues\")\n",
    "elif successful_runs > 0:\n",
    "    print(f\"   ‚úÖ {successful_runs} successful run(s) found!\")\n",
    "    print(\"   ‚Üí Ready for Phase 2 (full dataset training)\")\n",
    "\n",
    "if len(gradient_starved) > 0:\n",
    "    print(\"   ‚Üí Gradient starvation detected: check encoder architecture and loss computation\")\n",
    "\n",
    "if len(collapsed_encoder) > 0:\n",
    "    print(\"   ‚Üí Collapsed encoder detected: encoder not learning diverse representations\")\n",
    "\n",
    "# Best configuration to continue\n",
    "if len(df[df['health'] == '‚úÖ Healthy']) > 0:\n",
    "    healthy_df = df[df['health'] == '‚úÖ Healthy'].sort_values('train_exact_acc', ascending=False)\n",
    "    best_healthy = healthy_df.iloc[0]\n",
    "    print(f\"\\n‚ú® Best Configuration to Continue:\")\n",
    "    print(f\"   Encoder: {best_healthy['encoder_type']}\")\n",
    "    print(f\"   Layers: {best_healthy['encoder_layers']}\")\n",
    "    print(f\"   Max steps: {best_healthy['halt_max_steps']}\")\n",
    "    print(f\"   Explore prob: {best_healthy['halt_explore_prob']}\")\n",
    "    print(f\"   Current performance: {best_healthy['train_exact_acc']:.1f}%\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save the comparison table for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "import os\n",
    "\n",
    "output_file = f\"./results/{PROJECT}_comparison.csv\"\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
