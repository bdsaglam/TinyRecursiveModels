# ============================================================================
# ONLINE LEARNING ACT IMPLEMENTATION
# Previous bug: ACT halting logic caused encoder to be skipped after first batch
# Previous fix: Force halted=True each batch (works but only 1 ACT step)
#
# New implementation: TRUE ONLINE LEARNING (matches original paper)
# - Each ACT step: forward → backward → optim.step()
# - Encoder re-encodes demos fresh each step (gradients at every step)
# - Later ACT steps benefit from weight updates made by earlier steps
# - This matches original paper's training dynamics exactly
# - Eval still uses adaptive halting with halt_max_steps
#
# Default: num_act_steps=1, grad_clip_norm=1.0
# ============================================================================

# ============================================================================
# ACT STEPS ABLATION (on overfit test - 32 groups)
# Goal: Test impact of different ACT step counts during training
# ============================================================================

# A1: 1 ACT step (baseline - same as previous fix)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="A1_act_steps_1"

# A4: 4 ACT steps
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=4 max_train_groups=32 max_eval_groups=32 +project_name="mmi-714-debug" +run_name="A4_act_steps_4"

# A8: 8 ACT steps
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=8 max_train_groups=32 max_eval_groups=32 +project_name="mmi-714-debug" +run_name="A8_act_steps_8"

# A16: 16 ACT steps (matches eval halt_max_steps)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=16 max_train_groups=32 max_eval_groups=32 +project_name="mmi-714-debug" +run_name="A16_act_steps_16"

# ============================================================================
# GENERALIZATION EXPERIMENTS
# Goal: Achieve non-zero pass@K on test set (true generalization)
# Strategy: Train on full dataset (~560 puzzles), eval on held-out (~400 puzzles)
# Uses best ACT step count from ablation above (start with 1)
# ============================================================================

# G2: LPN Variational encoder - full training
[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_variational arch.num_act_steps=8 eval_interval=10000 +project_name="mmi-714-debug" +run_name="G2_lpn_var_full" [1454306]

# G3: LPN Standard encoder - full training
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_standard arch.num_act_steps=8 eval_interval=1000 +project_name="mmi-714-debug" +run_name="G3_lpn_std_full" [1452945]

# G1: Standard encoder - full training baseline
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=8 eval_interval=1000 +project_name="mmi-714-debug" +run_name="G1_standard_full"
