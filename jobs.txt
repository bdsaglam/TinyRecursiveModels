# ============================================================================
# BUG FIX VALIDATION (completed)
# Root cause: ACT halting logic caused encoder to be skipped after first batch
# Fix: Force halted=True at start of each batch to ensure fresh encoding
# Result: grad_clip_norm=1.0 achieves 96.7% train exact acc (best)
# ============================================================================

# E_fix_v1: Validate the gradient flow fix
# Result: 85.4% train exact accuracy, encoder_grad_norm > 0 ✓
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 grad_clip_norm=0.0 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_fix_v1"

# E_fix_gradclip: Same + gradient clipping (proven best for overfit)
# Result: 96.7% train exact accuracy ✓
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_fix_gradclip"

# E_fix_lpn_var: LPN Variational encoder (without grad clip - running)
# Result: 66.7% at last check (still running)
[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 grad_clip_norm=0.0 arch.encoder_type=lpn_variational epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_fix_lpn_var" [1371322]

# ============================================================================
# GENERALIZATION EXPERIMENTS
# Goal: Achieve non-zero pass@K on test set (true generalization)
# Strategy: Train on full dataset (~560 puzzles), eval on held-out (~400 puzzles)
# Default: grad_clip_norm=1.0 (now in config)
# ============================================================================

# G2: LPN Variational encoder - full training
# VAE regularization may help generalization (even if hurts memorization)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_variational epochs=100000 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-gen" +run_name="G2_lpn_var_full"

# G3: LPN Standard encoder - full training
# Deeper encoder (4 layers) without VAE overhead
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_standard epochs=100000 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-gen" +run_name="G3_lpn_std_full"

# G1: Standard encoder - full training baseline
# Tests if our simple 2-layer encoder can generalize
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 epochs=100000 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-gen" +run_name="G1_standard_full"
